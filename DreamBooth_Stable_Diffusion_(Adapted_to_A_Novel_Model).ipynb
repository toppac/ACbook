{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toppac/ACbook/blob/master/DreamBooth_Stable_Diffusion_(Adapted_to_A_Novel_Model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is designed to run smoothly on a Tesla T4 GPU.\n",
        "\n",
        "## Possible Issues\n",
        "\n",
        "If you encounter weird errors, that's probably because Colab is messing with your HTTP connection.\n",
        "\n",
        "Out of VRAM error won't happen if you are using the default settings and Tesla T4."
      ],
      "metadata": {
        "id": "BL3X7Rcz5lv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU7NuMAA2drw"
      },
      "outputs": [],
      "source": [
        "#@title Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "#@title Install Requirements\n",
        "!apt install -y -qq aria2 > /dev/null\n",
        "\n",
        "# !curl -LO https://github.com/huggingface/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "# use memory-optim version\n",
        "!git clone https://github.com/CCRcmcpe/diffusers.git\n",
        "\n",
        "TRAINER = \"diffusers/examples/dreambooth/train_dreambooth.py\"\n",
        "CONVERTER = \"diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py\"\n",
        "BACK_CONVERTER = \"diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py\"\n",
        "\n",
        "!pip install -U pip\n",
        "%pip install -q ./diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers==4.24.0 ftfy==6.1.1 bitsandbytes==0.35.4 omegaconf==2.2.3 einops==0.5.0 pytorch-lightning==1.7.7 gradio\n",
        "\n",
        "from subprocess import getoutput\n",
        "from IPython.display import HTML\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import os\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "  gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "  gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "  gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "  gpu = 'A100'\n",
        "\n",
        "while True:\n",
        "    try: \n",
        "        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
        "        break\n",
        "    except:\n",
        "        pass\n",
        "    print('It seems that your GPU is not supported at the moment')\n",
        "    time.sleep(5)\n",
        "\n",
        "if (gpu=='T4'):\n",
        "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "  \n",
        "elif (gpu=='P100'):\n",
        "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='V100'):\n",
        "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='A100'):\n",
        "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to wandb to watch training process (Optional, keep key empty if you want to skip)\n",
        "WANDB_KEY = \"\" #@param {type:\"string\"}\n",
        "if WANDB_KEY != \"\":\n",
        "  %pip install wandb\n",
        "  !wandb login $WANDB_KEY"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-pGMYCm_C0_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download model and convert it to HF format.\n",
        "\n",
        "#@markdown URL of the model (default: animefull-latest).\n",
        "URL = \"https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar\" #@param {type:\"string\"}\n",
        "#@markdown VAE weights. You may leave it empty.\n",
        "VAE_URL = \"https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animevae.pt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Model download path.\n",
        "SRC_PATH = \"/content/model-sd\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown SD to HF format convert output path.\n",
        "MODEL_NAME = \"/content/model-hf\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Download example training data (nahida)?\n",
        "download_example_traindata = True #@param {type:\"boolean\"}\n",
        "\n",
        "!echo \"Downloading model...\"\n",
        "!mkdir -p $SRC_PATH\n",
        "%pushd $SRC_PATH\n",
        "\n",
        "!aria2c --summary-interval=5 -x 6 --allow-overwrite=true -o data.tar $URL\n",
        "\n",
        "if VAE_URL != \"\":\n",
        "  !aria2c --summary-interval=5 -x 6 --allow-overwrite=true -o vae.pt $VAE_URL\n",
        "  vae_arg = f\"--vae_path {SRC_PATH}/vae.pt\"\n",
        "\n",
        "!tar xf data.tar \n",
        "!rm data.tar\n",
        "\n",
        "%popd\n",
        "!echo \"Done\"\n",
        "\n",
        "!echo \"Converting model...\"\n",
        "\n",
        "!python $CONVERTER --checkpoint_path $SRC_PATH/model.ckpt --original_config_file $SRC_PATH/config.yaml $vae_arg --dump_path $MODEL_NAME --scheduler_type ddim\n",
        "\n",
        "!echo \"Done\"\n",
        "\n",
        "!rm -r $SRC_PATH\n",
        "\n",
        "if download_example_traindata and not os.path.exists('instance-images'):\n",
        "  !mkdir instance-images\n",
        "  !aria2c --summary-interval=5 -x 6 --allow-overwrite=true https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/train-nahida.tar\n",
        "  !tar x -C instance-images -f train-nahida.tar\n",
        "  !rm train-nahida.tar\n",
        "  !rm -r instance-images/._train*\n"
      ],
      "metadata": {
        "id": "VYQ9Mt1pizKN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Prompt and Class Prompt\n",
        "\n",
        "What your training set is about|Instance prompt must contain|Class prompt should describe\n",
        "-|-|-\n",
        "A object/person|`[V]`|The object's type and/or characteristics\n",
        "A artist's style|`by [V]`|The common characteristics of the training set\n",
        "\n",
        "Where:\n",
        "* `[V]` is a *token* in CLIP's [vocabulary](https://huggingface.co/openai/clip-vit-large-patch14/raw/main/vocab.json) which is not meaningful to the model. `sks` is a great example.\n",
        "\n",
        "A common pitfall: like if you are training about a specific person with name `[N]`, you should NOT use `[N]` as `[V]`. Names have high chance of being separated (tokenized) to multiple tokens, which is possibly hazardous.\n",
        "\n",
        "Finally `[V]` will carry the new information learned by the model.\n",
        "\n",
        "### Examples\n",
        "\n",
        "Training about a female character:\n",
        "* Instance prompt: `sks 1girl`\n",
        "* Class prompt: `1girl`\n",
        "\n",
        "Training about hatsune miku (don't do this btw, model already knows):\n",
        "* Instance prompt: `masterpiece, best quality, sks 1girl, aqua eyes, aqua hair`\n",
        "* Class prompt: `masterpiece, best quality, 1girl, aqua eyes, aqua hair`\n",
        "\n",
        "Training about an artist's style on drawing female characters:\n",
        "* Instance prompt: `1girl, by sks`\n",
        "* Class prompt: `1girl`"
      ],
      "metadata": {
        "id": "sDQlFaiN34Ih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "\n",
        "#@markdown ## Train set\n",
        "\n",
        "INSTANCE_PROMPT = \"masterpiece, best quality, sks 1girl\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Training set path (images of the subject).\n",
        "INSTANCE_DIR = \"/content/instance-images\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Class set\n",
        "CLASS_PROMPT = \"masterpiece, best quality, 1girl\" #@param {type:\"string\"}\n",
        "CLASS_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type:\"string\"}\n",
        "CLASS_DIR = \"/content/class-images\" #@param {type:\"string\"}\n",
        "NUM_CLASS_IMAGES = 100 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ## Previewing\n",
        "#@markdown Prompt for saving samples.\n",
        "SAVE_SAMPLE_PROMPT = \"masterpiece, best quality, sks 1girl, looking at viewer\" #@param {type:\"string\"}\n",
        "SAVE_SAMPLE_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Output\n",
        "#@markdown If model weights should be saved directly to google drive (takes around 2-3 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"output\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "## Advanced\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n",
        "\n",
        "If you are using a GPU better than Tesla T4, remove `--gradient_checkpointing` from the arguments to improve training speed, also consider increasing `train_batch_size` for less overhead and better regularization.\n",
        "\n",
        "Remove `--use_8bit_adam` flag for full precision optimizer. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB. Somewhat unreasonable to do, but if you are encountering issues with the reduced precision, oh well.\n",
        "\n",
        "### Multiple Concepts\n",
        "\n",
        "You can set up a `concepts_list.json` like:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of a woman wearing sks dress\",\n",
        "        \"class_prompt\":         \"photo of a woman wearing dress\",\n",
        "        \"instance_data_dir\":    \"data/wrap_dress\",\n",
        "        \"class_data_dir\":       \"data/dress\"\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of sks woman\",\n",
        "        \"class_prompt\":         \"photo of a woman\",\n",
        "        \"instance_data_dir\":    \"data/woman1\",\n",
        "        \"class_data_dir\":       \"data/woman_class\"\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "And use it with `--concepts_list concepts_list.json`. Can let model learn multiple concepts at the same time. Currently not compatible with Variable Prompts.\n",
        "\n",
        "### Variable Prompts\n",
        "\n",
        "For each image (`[X].png` / `[X].jpg`) in data set, put an additional `[X].txt` containing corresponding prompt with it. Then set `READ_PROMPT_FROM_TXT`. Both train set and class set supports this.\n",
        "\n",
        "Prompt read from txt `[PX]` will be inserted to the prompt you set `[P]` in train args. By default, it is inserted like `[PX] [P]`.\n",
        "\n",
        "With Variable Prompts enabled and prior preservation loss disabled (`PRIOR_PRESERVATION`), the training process is effectively an equivalent to standard finetuning.\n",
        "\n",
        "### Aspect Ratio Bucketing\n",
        "\n",
        "Used by NovelAI when they train their model. In a nutshell, it sets variable training resolution, eliminating the need of cropping dataset manually to 1:1 while still preserving information well. Brings better result especially when generating images with aspect ratio ≠ 1.\n",
        "\n",
        "Cost is it slows down training and requires slightly more VRAM. Tested using it with batch size = 2 on Tesla T4 is fine.\n",
        "\n",
        "### Optimizer\n",
        "\n",
        "The default is int8 AdamW. Works well. If you want to use SGDM, prepare to get into troubles like model does not give much different results after many steps.\n",
        "\n",
        "### Storage Issue\n",
        "\n",
        "To save Colab from out of storage, by default we save unet weights using FP16 (`--save_unet_half`).\n",
        "\n",
        "If you enabled wandb, you can add `--wandb_artifact` to upload weights to wandb. Optionally, `--rm_after_wandb_saved` can let weights be removed after uploading. (Because Colab somewhat actively mess with connections, this is disabled by default.)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Advanced Parameters\n",
        "MAX_TRAIN_STEPS = 2000 #@param {type:\"number\"}\n",
        "SAVE_INTERVAL = 500 #@param {type:\"number\"}\n",
        "SEED = 114514 #@param {type:\"number\"}\n",
        "#@markdown ## Data Processing\n",
        "RESOLUTION = 512 #@param {type:\"slider\", min:64, max:2048, step:28}\n",
        "ASPECT_RATIO_BUCKETING = False #@param {type:\"boolean\"}\n",
        "READ_PROMPT_FROM_TXT = \"no\" #@param [\"no\", \"instance\", \"class\", \"both\"] {allow-input: false}\n",
        "#@markdown ## Forward Pass\n",
        "TRAIN_BATCH_SIZE = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "GRADIENT_ACCUMULATION_STEPS = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "CLIP_SKIP = 2 #@param {type:\"slider\", min:1, max:6, step:1}\n",
        "MIXED_PRECISION = \"fp16\" #@param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "#@markdown ## Optimizer / Backward Pass\n",
        "OPTIMIZER = \"adamw_8bit\" #@param [\"adamw\", \"adamw_8bit\", \"adamw_ds\", \"sgdm\", \"sgdm_8bit\"] {allow-input: false}\n",
        "LEARNING_RATE = 5e-6 #@param {type:\"number\"}\n",
        "LR_SCHEDULER = \"cosine_with_restarts\"  #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "LR_WARMUP_STEPS = 100 #@param {type:\"number\"}\n",
        "LR_CYCLES = 1 #@param {type:\"number\"}\n",
        "LAST_EPOCH = -1 #@param {type:\"number\"}\n",
        "SCALE_LR = True #@param {type:\"boolean\"}\n",
        "PRIOR_PRESERVATION = True #@param {type:\"boolean\"}\n",
        "PRIOR_LOSS_WEIGHT = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown ## Inference (Class Set Generation / Sample Images Generation)\n",
        "INFER_STEPS = 28 #@param {type:\"integer\"}\n",
        "GUIDANCE_SCALE = 11 #@param {type:\"integer\"}\n",
        "SAMPLE_N = 4  #@param {type:\"integer\"}\n",
        "INFER_BATCH_SIZE = 2 #@param {type:\"slider\", min:1, max:10, step:1}"
      ],
      "metadata": {
        "id": "3EAAQDUC_3lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HF Accelerate Config\n",
        "%%bash\n",
        "\n",
        "mkdir -p ~/.cache/huggingface/accelerate\n",
        "\n",
        "cat > ~/.cache/huggingface/accelerate/default_config.yaml <<- EOM\n",
        "compute_environment: LOCAL_MACHINE\n",
        "deepspeed_config: {}\n",
        "distributed_type: 'NO'\n",
        "downcast_bf16: 'no'\n",
        "fsdp_config: {}\n",
        "machine_rank: 0\n",
        "main_process_ip: null\n",
        "main_process_port: null\n",
        "main_training_function: main\n",
        "mixed_precision: fp16\n",
        "num_machines: 1\n",
        "num_processes: 1\n",
        "use_cpu: false\n",
        "EOM"
      ],
      "metadata": {
        "id": "G26jPrUSqWUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "wandb_arg = \"--wandb\" if WANDB_KEY != \"\" else \"\"\n",
        "scale_lr_arg = \"--scale_lr\" if SCALE_LR else \"\"\n",
        "ppl_arg = f\"--with_prior_preservation --prior_loss_weight={PRIOR_LOSS_WEIGHT}\" if PRIOR_PRESERVATION else \"\"\n",
        "read_prompt_arg = f\"--read_prompt_from_txt {READ_PROMPT_FROM_TXT}\" if READ_PROMPT_FROM_TXT != \"no\" else \"\"\n",
        "arb_arg = \"--use_aspect_ratio_bucket --debug_arb\" if ASPECT_RATIO_BUCKETING else \"\"\n",
        "\n",
        "\n",
        "!accelerate launch $TRAINER \\\n",
        "  --instance_data_dir \"{INSTANCE_DIR}\" \\\n",
        "  --instance_prompt \"{INSTANCE_PROMPT}\" \\\n",
        "  --pretrained_model_name_or_path \"{MODEL_NAME}\" \\\n",
        "  --pretrained_vae_name_or_path \"{MODEL_NAME}/vae\" \\\n",
        "  --output_dir \"{OUTPUT_DIR}\" \\\n",
        "  --seed=$SEED \\\n",
        "  --resolution=$RESOLUTION \\\n",
        "  --optimizer \"{OPTIMIZER}\" \\\n",
        "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
        "  --learning_rate=$LEARNING_RATE \\\n",
        "  --lr_scheduler=$LR_SCHEDULER \\\n",
        "  --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
        "  --lr_cycles=$LR_CYCLES \\\n",
        "  --last_epoch=$LAST_EPOCH \\\n",
        "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
        "  --save_interval=$SAVE_INTERVAL \\\n",
        "  --class_data_dir \"{CLASS_DIR}\" \\\n",
        "  --class_prompt \"{CLASS_PROMPT}\" --class_negative_prompt \"{CLASS_NEGATIVE_PROMPT}\" \\\n",
        "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
        "  --save_sample_prompt \"{SAVE_SAMPLE_PROMPT}\" --save_sample_negative_prompt \"{SAVE_SAMPLE_NEGATIVE_PROMPT}\" \\\n",
        "  --n_save_sample=$SAMPLE_N \\\n",
        "  --infer_batch_size=$INFER_BATCH_SIZE \\\n",
        "  --infer_steps=$INFER_STEPS \\\n",
        "  --guidance_scale=$GUIDANCE_SCALE \\\n",
        "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
        "  --gradient_checkpointing \\\n",
        "  --save_unet_half \\\n",
        "  --mixed_precision \"{MIXED_PRECISION}\" \\\n",
        "  --clip_skip=$CLIP_SKIP \\\n",
        "  $wandb_arg $scale_lr_arg $ppl_arg $read_prompt_arg $arb_arg\n",
        "\n",
        "# disabled: --not_cache_latents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@markdown Which step number to use.\n",
        "use_checkpoint = '2000' #@param {type:\"string\"}\n",
        "#@markdown Id of which run to use (empty = latest run).\n",
        "run_id = '' #@param {type:\"string\"}\n",
        "\n",
        "if not run_id:\n",
        "  runs = [d for d in Path(OUTPUT_DIR).iterdir() if d.is_dir()]\n",
        "  runs.sort(lambda d: d.stat().st_ctime, reverse=True)\n",
        "  run_id = runs[0].name\n",
        "\n",
        "ckpt_path = f'{OUTPUT_DIR}/{run_id}/{use_checkpoint}/model.ckpt'\n",
        "\n",
        "# You can add --vae and --text_encoder if you want.\n",
        "!python \"{BACK_CONVERTER}\" --model_path \"{OUTPUT_DIR}/{run_id}/{use_checkpoint}\" --checkpoint_path $ckpt_path \\\n",
        "  --unet_dtype fp16\n",
        "\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "#@markdown Run Gradio UI for generating images.\n",
        "use_checkpoint = '2000' #@param {type:\"string\"}\n",
        "model_path = os.path.join(OUTPUT_DIR, use_checkpoint)\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
        "g_cuda = None\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of sks guy, digital painting\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}